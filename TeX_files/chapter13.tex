\chapter{Quality Control}


\begin{flushright}
	\textit{``Control charts are bad."} 
\end{flushright}

\section{Introduction}

Variation in a process intended to generate successive samples that are identical is an unavoidable part of real-world processes. This variation may or may not be attributable to some cause not intrinsic to the process itself. 

Investigating whether a process is occurring \textit{in control}, involves the use of \textit{control charts} which are described through subgroup statistics.

Statistics of the subgroups are then compared against a lower and upper control limit, which make up the control chart. The subgroup means and standard deviations, taken together or in isolation, are most often used to create these control charts.

\section{Average values - $ \overline{x} $ control chart}

Let the process produce items that are IID as normal RVs with parameters $ (\mu, \sigma^2) $. Let the set of data points $ \{X_i\} $ be subdivided into groups os size $ n $, with the choice of $ n $ such that variation is likely across and not within subgroups.

Consider the mean of the $ n $ elements in each subgroup $ \overline{X_i} $.

\begin{align}
	\overline{X_i} &= \frac{X_1 + \dots + X_n}{n}
\end{align}

Rearranging into a standard normal variable using the mean and variance,

\begin{align}
	\mathbb{E}[\overline{X_i}] &= \mu \\
	%
	\mathrm{Var}(\overline{X_i}) &= \frac{\sigma^2}{n} \\
	%
	\frac{\overline{X_i} - \mu}{\sqrt{\sigma^2 / n}} &\sim Z
\end{align}

All products with a z-score less than 3 are considered acceptable by convention. This yields the lower and upper control limits as follows,

\begin{align}
	Z &\in [-3, 3] \\
	%
	\overline{X_i} &\in \left[ \mu \pm \frac{3\sigma}{\sqrt{n}} \right] \\
	%
	\overline{X_i} &\in [\texttt{LCL},\ \texttt{UCL}]
\end{align}

The process can now be declared out of control when a subgroup first violates the acceptable region as defined above.

The central limit theorem enables the use of the above procedure at the subgroup level even when the underlying probability distribution is not normal at the product level.

There is a probability $ P\{|Z| > 3\} = 0.27\% $ of falsely declaring a process out of control. Since the most common cause of the process going out of control is a shift in the product mean value $ \mu \to \mu + a $.

\begin{align}
	\mu &\to \mu + a \qquad \text{and} \qquad \sigma^2 \to \sigma^2 \\
	%
	\frac{\overline{X} - (\mu + a)}{\sigma/\sqrt{n}} &\sim Z \\
	%
	P\{Z \in \left[\texttt{LCL},\ \texttt{UCL}\right]\} &= \Phi\left(3 - \frac{a}{\sigma/\sqrt{n}}\right)
\end{align}

This gives the probability of detecting loss of control when the process mean shifts by $ a $ keeping the same variance.

\textit{Unknown mean and variance} : If historical data on $ (\mu, \sigma^2) $ is unavailable, it can be estimated from a set of $ k $ initial subgroups such that $ k \geq 20,\ nk \geq 100 $.

The grand mean ($ \overline{\overline{X}} $) of the first $ k $ subgroup means is an estimator of $ \mu $. In order to find an estimator of $ \sigma^2 $, the sample variances of these subgroups are used along with a bias correction.

\begin{align}
	\overline{\overline{X}} &= \frac{\overline{X_1} + \dots + \overline{X_k}}{k} \\
	%
	S_i^2 &= \sum\limits_{j = 1}^{n} \frac{(X_{j + (i-1)n} - \overline{X_i})^2}{(n-1)} \\
	%
	\overline{S} &= \frac{S_1 + \dots + S_k}{k}
\end{align}

Using the fact that the set $ \{S_i\} $ are independent, and that the sample variance can be rearranged into a $ \chi^2 $-RV, 

\begin{align}
	\mathbb{E}[\overline{S}] &= \frac{\mathbb{E}[S_1] + \dots + \mathbb{E}[S_k]}{k} = \mathbb{E}[S_1] \\
	%
	(n-1)\ \frac{S_1^2}{\sigma^2} &\sim \chi^2_{n-1} \\
	%
	\mathbb{E}\left[\sqrt{\chi^2_{n-1}}\right] &= \frac{\sqrt{2}\ \Gamma(\tfrac{n}{2})}{\Gamma(\tfrac{n-1}{2})}
\end{align}

The known result of the expectation value of a chi-RV is used here to indirectly find $ \mathbb{E}[S_1] $.

\begin{align}
	\mathbb{E}[S_1] &= \frac{\sigma}{\sqrt{(n-1)}}\ \frac{\sqrt{2}\ \Gamma(\tfrac{n}{2})}{\Gamma(\tfrac{n-1}{2})} = \sigma\ c(n)
\end{align}

The function $ c(n) $ is used as a shorthand above for convenience. As a cross-check, the control limits produced by these $ k $ subgroups are used to check if any of them violate these limits. If so, these subgroups are eliminated and the process repeated until there are no violators.

After having confirmed that the process has remained in control throughout using the above trimming and recalculation procedure, the unbiased estimator of the standard deviation $ \sigma $ is now formulated as,

\begin{align}
	\sigma &\approxeq \frac{\overline{S}}{c(n)} \qquad \text{with} \qquad \mathbb{E}\left[\frac{\overline{S}}{c(n)}\right] = \sigma
\end{align}

The grand sample variance (using all $ nk $ elements) is not as good an estimator of $ \sigma $ as the one used above, because of possible shifts in the mean value between subgroups while preserving variance.

\section{S-control charts}

The detection of changes in population variance requires a control chart to be formulated based on the sample variances from subgroups, which form the set $ \{S_i\} $.

\begin{align}
	S_i^2 &= \sum\limits_{j = 1}^{n} \frac{(X_{j + (i-1)n} - \overline{X_i})^2}{(n-1)} \\ \nonumber 
	%
	\mathbb{E}[S_i] &= \sigma\ c(n) \qquad \text{and} \qquad \mathrm{Var}(S_i) = \sigma^2 [1 - c^2(n)]
\end{align}

Using a z-score of 3 once again as convention, $ P\{\chi_{n-1} \in [-3, 3]\} > 99\% $ yields the upper and lower control values as, 

\begin{align}
	\frac{S_i - \mathbb{E}[S_i]}{\sqrt{\mathrm{Var}(S_i)}} &\in [-3, 3] \\
	%
	S_i &\in [\sigma c(n) \pm 3 \sigma \sqrt{1 - c^2(n)}]
\end{align} 

In the absence of historical data about $ \sigma^2 $, it can be replaced by its unbiased estimator $ \overline{S} / c(n) $ as defined above, alongside the iterative procedure of cross-checking against the subgroups used to perform the computation and discarding any subgroups that happen to violate the control limits.

\section{Control charts for the fraction defective}

Consider the real world problem where successive products produced by a process do not have a continuously varying observable characteristic, and are instead only classifiable as defective or acceptable.

Define the fraction of defective items in each subgroup $ F $ using the number of defective items $ X $. Also, let $ X $ be a binomial RV with parameters $ (n, p) $.

\begin{align}
	F &= \frac{X}{n} \\[1ex]
	%
	\mathbb{E}[F] &= \frac{\mathbb{E}[X]}{n} = \frac{np}{n} = p \\[1ex]
	%
	\mathrm{Var}(F) &= \frac{\mathrm{Var}(X)}{n^2} = \frac{p(1-p)}{n}
\end{align}

Once again the control limits can be defined using a z-score of 3 as,

\begin{align}
	F &\in [\texttt{LCL}, \texttt{UCL}] \nonumber \\
	%
	F &\in \left[p \pm 3\ \sqrt{\frac{p(1-p)}{n}}\right]
\end{align}

Since $ p $, the probability of an individual product being defective is usually very small, $ n $ has to be large to ensure that each subgroup has a reasonable chance of more than one defect.

In the absence of historical information about $ F $, it can be estimated from a set of $ k $ initial subgroups ($ k \geq 20 $) using an estimator $ \overline{F} $ of $ p $

\begin{align}
	\overline{F} &= \frac{F_1 + \dots + F_k}{k} = \frac{\text{total defective items}}{\text{total items}} \\
	%
	F &\in \left[\overline{F} \pm 3\ \sqrt{\frac{\overline{F}(1-\overline{F})}{n}}\right]
\end{align}

The iterative cross-check and trimming of subgroups also needs to be carried out until a set of initial subgroups is reached without any violations of the control limits.

The above control limits do not discriminate between increases and decreases in quality, only the deviation from current quality.

\section{Control charts for the number of defects}


Using the Poisson approximation to the binomial RV when $ n $ is large and $ p $ small, the number of defective products in each subgroup can be assumed to be independently Poisson distributed with mean $ \lambda $. 


If the number of defective items per subgroup is $ X_i $, the control limits are defined as

\begin{align}
	\mathbb{E}[X_i] &= \mathrm{Var}(X_i) = \lambda \\
	%
	X_i &\in \left[ \lambda \pm 3\ \sqrt{\lambda} \right] \\
	%
	\overline{X} &= \frac{\sum_i X_i}{k}
\end{align} 

When information about $ \lambda $ is not available, it can be replaced by its estimator $ \overline{X} $. 

In order to have good sensitivity when it comes to detecting loss of control, it might be necessary to pool together multiple subgroups into larger units such that the number of defects per unit is at least 25.

This is because a small value of $ X_i $ makes it possible for the process to remain out of control undetected for a significant duration if the number of size $ n $ of the subgroup is too small.

\section{Control charts for detecting drifts in population mean}

The $ \overline{X} $ control charts previously covered suffer from insensitivity to small changes in $ \mu $, since each plotted value has to be computed from a single subgroup and thus have a large variance.

By computing a subgroup average using some information from other subgroups as well, it is possible to greatly reduce the variance in each data point and thus increase sensitivity to small drifts in $ \mu $.

\textit{Moving average control charts} : For a span size $ k $, the moving average $ M_t $ at time $ t $ is simply the average of the $ k $ most recent subgroups.

\begin{align}
	M_t &= \frac{\overline{X}_t + \overline{X}_{t-1} + \dots + \overline{X}_{t-k+1}}{k} \\[1ex]
	%
	M_{t+1} &= M_t + \frac{\overline{X}_{t+1} - \overline{X}_{t-k+1}}{k}
\end{align}

The incremental relation above simply uses the difference between successive moving averages and the non-shared terms as a correction. By convention, the moving average for $ t < k $, simply uses the first $ t $ subgroups. These two cases separately have control limits defined as,

\begin{align}
	\text{Case} &: t < k & \text{Case} &: t \geq k \nonumber \\
	%
	\mathbb{E}[M_T] &= \mu & \mathbb{E}[M_T] &= \mu \\
	%
	\mathrm{Var}(M_t) &= \frac{\sigma^2}{nt} & \mathrm{Var}(M_t) &= \frac{\sigma^2}{nk} \\
	%
	M_t &\in \left[\mu \pm \frac{3\sigma}{\sqrt{nt}}\right] & M_t &\in \left[\mu \pm \frac{3\sigma}{\sqrt{nk}}\right] 
\end{align} 

As seen above, span size $ k $ is inversely related to the size of the change in mean that is to be detected.


\textit{Exponentially weighted moving-average control charts} : As opposed to the simple equal weight system described above, a weighting method that overvalues recent subgroups and undervalues older subgroups can be accomplished using an exponentially decaying weight sequence.

\begin{align}
	W_t &= \alpha\ \overline{X}_t + (1 - \alpha)\ W_{t-1} \\
	%
	W_0 &= \mu \\
	%
	\alpha &\in \left(0, 1\right)
\end{align}

From the above definition, the weights of older subgroups decay by a constant factor of $ (1-\alpha) $. Using the fact that $ a^x = \exp \left[x\ \ln (a)\right] $, 

\begin{align}
	\alpha\ (1-\alpha)^{i-1} &= \overline{\alpha}\ \exp(-\beta i) \\
	%
	\overline{\alpha} = \frac{\alpha}{(1 - \alpha)} \qquad &\text{and} \qquad \beta = -\ln(1-\alpha)
\end{align}

To find the expectation value of $ W_t $, the fact that all of the subgroup averages are independent normal RVs with parameters $ (\mu, \sigma^2/n) $ is used. 

\begin{align}
	\mathbb{E}[W_t] &= \mu \alpha \left[1 + (1-\alpha) + (1-\alpha)^2 + \dots + (1-\alpha)^{t-1}\right] \nonumber \\
	%
	& + \mu (1-\alpha)^t \\[1ex]
	%
	\mathbb{E}[W_t] &= \frac{\mu \alpha [1 - (1-\alpha)^t]}{1 - (1 - \alpha)} + \mu (1-\alpha)^t \nonumber \\
	%
	\mathbb{E}[W_t] &= \mu \\
	%
	\mathrm{Var}(W_t) &= \frac{\sigma^2}{n}\ \frac{\alpha\ [1 - (1-\alpha)^{2t}]}{(2 - \alpha)}
\end{align}

Under the limit of large $ t $, if the process has remained in control, the control limits are given by,

\begin{align}
	\mathbb{E}[W_t] &= \mu & \mathrm{Var}(W_t) &\approxeq \frac{\sigma^2}{n}\ \frac{\alpha}{(2-\alpha)}\nonumber \\[1ex]
	%
	W_t &\in \left[\mu \pm 3\sigma\ \sqrt{\frac{\alpha}{n(2-\alpha)}}\right]
\end{align}

Note the similarity between the control limits using a uniformly weighted moving average of span $ k $, with $ k = (2-\alpha)/\alpha $

\textit{Cumulative sum control charts} : Keeping the same $ \{\overline{X}_i\} $ symbols from previous sections, define a new RV to monitor increases in population mean, 

\begin{align}
	Y_i &= \overline{X}_i - \mu - \frac{d\sigma}{\sqrt{n}} \qquad \text{with} \qquad d > 0,\ i \in \{1, m\} \\
	%
	\mathbb{E}[Y_i] &= - \frac{d\sigma}{\sqrt{n}} < 0
\end{align}

Now, define an iteratively computed cumulative sum $ \{S_i\} $ such that,

\begin{align}
	S_0 &= 0 \\
	%
	S_{i+1} &= \max(S_i + Y_{i+1},\ 0)
\end{align}

The above fail-safe is intended to reset $ S_{i+1} $ back to zero whenever it becomes negative. Since each $ Y_i $ has a negative expected value, the cumulative sum is expected to be a negative number. 

However, a positive fail condition using $ B > 0 $, can be set up to declare the process out of control if the cumulative sum exceeds a large positive value,

\begin{align}
	S_j > \frac{B\sigma}{\sqrt{n}} \qquad \to \qquad \text{out of control}
\end{align}

The utility of the reset to zero is to keep the system capable of detecting loss of control even after a large number of time steps have passed with the system having remained in control.

A complementary cumulative sum chart measuring the negatives of subgroup averages is also employed simultaneously to keep track of decreases in $ \mu $. It works using the exact same mechanism but in the opposite direction.








